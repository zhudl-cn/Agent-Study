# 📝 llama.cpp 学习复习测验报告

> 测验日期：2026-01-27
> 复习范围：Day 2 - Day 4

---

## 📊 成绩总览

| 模块 | 满分 | 得分 | 正确率 |
|------|------|------|--------|
| Day 2: GGUF 文件结构 | 30 | 29 | 97% |
| Day 3: llama_decode 调用链 | 30 | 25 | 83% |
| Day 4: 量化原理 | 40 | 28 | 70% |
| **总计** | **100** | **82** | **82%** |

### 评级：🌟🌟🌟🌟 良好

---

## 📋 详细题目与答案

### Day 2: GGUF 文件结构 (29/30)

#### 问题 1: GGUF Header 字段 ✅ (9/10)

**你的回答：**
- 文件类型，版本号，标量的数量，kv的数量
- char[4], int32_t, int64_t, int64_t

**正确答案：**
| 字段 | 数据类型 | 说明 |
|------|----------|------|
| Magic | char[4] | "GGUF" 文件标识 |
| Version | uint32_t | 版本号 (当前 v3) |
| n_tensors | uint64_t | **张量**数量 |
| n_kv | uint64_t | KV 对数量 |

**扣分原因：** "标量" 应为 "张量"，unsigned 更精确

---

#### 问题 2: 小端序解析 ✅ (10/10)

**你的回答：**
- 291，表示张量的数量
- 小端存储，因为 x86 架构

**正确答案：**
```
2301 0000 0000 0000 (小端序)
→ 0x0000000000000123
→ 291 (张量数量)
```

**评价：** 完全正确！

---

#### 问题 3: Alignment=32 的作用 ✅ (10/10)

**你的回答：**
- 数据对齐，提高读取性能
- SIMD 256 指令有关

**正确答案：**
- AVX2 SIMD 使用 256-bit (32 bytes) 寄存器
- 对齐数据可高效执行向量指令

**评价：** 完全正确！

---

### Day 3: llama_decode 调用链 (25/30)

#### 问题 4: 调用链追踪 ✅ (8/10)

**你的回答：**
- llama_decode → ctx->decoder → build graph → compute

**正确答案：**
```
llama_decode()
└── ctx->decode()          ← 不是 decoder
    └── process_ubatch()
        ├── build_graph()
        └── graph_compute()
```

**扣分原因：** `ctx->decoder` 应为 `ctx->decode()`

---

#### 问题 5: Self-Attention 的两次 GEMM ✅ (9/10)

**你的回答：**
- 第一次计算权重
- 第二次计算权重值下，实际的 value 值

**正确答案：**
| GEMM | 计算内容 | 代码位置 |
|------|----------|----------|
| 第一次 | Q × K^T → 注意力分数 | llama-graph.cpp:1655 |
| 第二次 | softmax(scores) × V → 加权输出 | llama-graph.cpp:1699 |

**评价：** 理解正确，术语可更精确

---

#### 问题 6: 计算图设计优势 ✅ (8/10)

**你的回答：**
1. 一次构建，多次调用
2. 可以实现共享（多线程共享计算图）

**正确答案：**
1. ✅ 复用：减少重复分析开销
2. ⚠️ 共享：更常见的说法是"内存复用"
3. 📝 其他优势：并行调度、跨设备调度、算子融合

---

### Day 4: 量化原理 (28/40)

#### 问题 7: block_q4_K 结构 ⚠️ (7/10)

**你的回答：**
- 256 个权重 ✅
- 1152 个字节 ❌
- 4.5 bpw ✅

**正确答案：**
```
block_q4_K:
├── d:      2 bytes
├── dmin:   2 bytes
├── scales: 12 bytes
└── qs:     128 bytes
────────────────────
总计: 144 bytes (不是 1152！)

bpw = 144 × 8 / 256 = 4.5
```

**扣分原因：** 1152 是 bits，不是 bytes

---

#### 问题 8: 两层量化 ✅ (8/10)

**你的回答：**
- 一层是标量量化，一层是将 scale 和 min 也量化
- 无需额外存储空间

**正确答案：**
```
第一层 (Super-block): d, dmin (FP16) ← 精确
第二层 (Sub-block): scale, min (6-bit) ← 粗略量化
```

**好处：** 节省空间（从 8 bytes 降到 1.5 bytes/sub-block）

**扣分原因：** "标量" → "权重"，"无需" → "减少"

---

#### 问题 9: 反量化公式 ❌ (3/10)

**你的回答：**
- qs / scale

**正确答案：**
```
原始值 = d × scale × q - dmin × min
```

**后续讨论：** 你成功推导出了量化公式！
```
q = (原始值 + dmin × min) / (d × scale)
```

**扣分原因：** 初始回答不完整（但后续理解正确）

---

#### 问题 10: GEMM 性能分析 ✅ (10/10)

**你的回答：**
- 90%
- 因为主要涉及计算消耗处理器和内存

**正确答案：**
- 占比：80-90%
- 原因：
  1. 计算密集：O(M×N×K) 复杂度
  2. 频率高：每层多次 GEMM
  3. 数据密集：大量权重读取

---

## 💡 知识点掌握情况

### ✅ 掌握良好

| 知识点 | 表现 |
|--------|------|
| GGUF Header 结构 | 字段和类型基本正确 |
| 小端序原理 | 能正确解析二进制 |
| AVX2 与对齐 | 理解 SIMD 优化原理 |
| llama_decode 调用链 | 核心流程清晰 |
| Self-Attention 公式 | 理解两次 GEMM 的作用 |
| 两层量化概念 | 理解层级结构 |
| GEMM 性能瓶颈 | 知道占比和原因 |

### ⚠️ 需要加强

| 知识点 | 问题 | 建议 |
|--------|------|------|
| **张量 vs 标量** | 多次混淆 | 张量 = 多维数组，标量 = 单个数值 |
| **block_q4_K 大小** | bits/bytes 混淆 | 记住：144 bytes = 1152 bits |
| **反量化公式** | 初始回答不完整 | 多练习：`d × scale × q - dmin × min` |

---

## 📈 学习建议

### 1. 强化记忆

```
🔴 必背公式：
反量化：原始值 = d × scale × q - dmin × min
量化：  q = round((原始值 + dmin × min) / (d × scale))

🔵 必背数字：
block_q4_K = 144 bytes → 256 weights → 4.5 bpw
GEMM 占比 ≈ 80-90%
```

### 2. 动手实践

```bash
# 查看反量化函数实现
grep -n "dequantize_row_q4_K" ggml/src/ggml-quants.c

# 用 xxd 验证 GGUF 结构
xxd -l 24 models/your_model.gguf
```

### 3. 下一步计划

- [ ] Day 5: KV Cache 探秘
- [ ] 实践：用代码打印 KV Cache 显存占用
- [ ] 深入：阅读 `ggml-quants.c` 中的反量化实现

---

## 🏆 总结

**整体评价：良好！**

你对 llama.cpp 的核心概念有了扎实的理解：
- GGUF 文件结构 ✅
- 推理调用链 ✅
- 计算图设计 ✅
- 量化原理（需要继续加强）

特别值得表扬的是你主动推导量化公式的思考过程，这说明你不只是在记忆，而是在**理解**！

---

*📅 下次复习建议：进入 Day 5 学习前，复习反量化公式*

