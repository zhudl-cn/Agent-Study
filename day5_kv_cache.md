# Day 5: KV Cache æ¢ç§˜

> å­¦ä¹ æ—¥æœŸï¼š2026-01-27
> ç›®æ ‡ï¼šè®¡ç®—å¹¶æ‰“å°å½“å‰ Context ä½¿ç”¨çš„ KV Cache æ˜¾å­˜å¤§å°

---

## ä¸€ã€ä»€ä¹ˆæ˜¯ KV Cacheï¼Ÿ

### 1.1 é—®é¢˜èƒŒæ™¯

åœ¨è‡ªå›å½’ç”Ÿæˆï¼ˆAutoregressive Generationï¼‰ä¸­ï¼Œæ¯ç”Ÿæˆä¸€ä¸ªæ–° token éƒ½éœ€è¦ï¼š

```
è¾“å…¥: [t1, t2, t3, ..., tn, t_new]
          â†“
    è®¡ç®— Q, K, V
          â†“
    Attention(Q, K, V)
          â†“
è¾“å‡º: t_new+1
```

**é—®é¢˜**ï¼šæ¯æ¬¡éƒ½è¦é‡æ–°è®¡ç®—æ‰€æœ‰å†å² token çš„ K å’Œ Vï¼Œéå¸¸æµªè´¹ï¼

### 1.2 è§£å†³æ–¹æ¡ˆï¼šç¼“å­˜ K å’Œ V

```
ç¬¬ 1 æ¬¡: è®¡ç®— [K1, V1] â†’ å­˜å…¥ç¼“å­˜
ç¬¬ 2 æ¬¡: è®¡ç®— [K2, V2] â†’ å­˜å…¥ç¼“å­˜ï¼Œå¤ç”¨ [K1, V1]
ç¬¬ n æ¬¡: åªè®¡ç®— [Kn, Vn] â†’ å¤ç”¨æ‰€æœ‰å†å² [K1..Kn-1, V1..Vn-1]
```

**KV Cache = å­˜å‚¨å†å² token çš„ Key å’Œ Value å‘é‡**

### 1.3 ä¸ºä»€ä¹ˆåªç¼“å­˜ K å’Œ Vï¼Œä¸ç¼“å­˜ Qï¼Ÿ

**ç­”æ¡ˆ**ï¼š
- Q åªéœ€è¦å½“å‰ token çš„ï¼Œæ¯æ¬¡æ–°è®¡ç®—å³å¯ï¼Œä¸éœ€è¦å†å²
- K å’Œ V éœ€è¦æ‰€æœ‰å†å² token çš„ï¼Œå¿…é¡»ç¼“å­˜ä»¥é¿å…é‡å¤è®¡ç®—

```
ç”Ÿæˆç¬¬ n ä¸ª tokenï¼š
- Q_nï¼ˆå½“å‰ï¼‰Ã— [K_1...K_n]^Tï¼ˆå†å²+å½“å‰ï¼‰â†’ æ³¨æ„åŠ›åˆ†æ•°
- æ³¨æ„åŠ›åˆ†æ•° Ã— [V_1...V_n]ï¼ˆå†å²+å½“å‰ï¼‰â†’ è¾“å‡º
```

---

## äºŒã€KV Cache çš„ Shape

### 2.1 å•å±‚ KV Cache

```
K cache shape: [batch_size, n_heads, seq_len, head_dim]
V cache shape: [batch_size, n_heads, seq_len, head_dim]
```

æˆ–è€…ï¼ˆllama.cpp çš„å­˜å‚¨æ–¹å¼ï¼‰ï¼š

```
K cache shape: [n_layers, seq_len, n_kv_heads, head_dim]
V cache shape: [n_layers, seq_len, n_kv_heads, head_dim]
```

### 2.2 å…³é”®å‚æ•°

| å‚æ•° | å«ä¹‰ | Qwen2.5-0.5B çš„å€¼ |
|------|------|-------------------|
| n_layers | Transformer å±‚æ•° | [å¾…å¡«å†™] |
| n_heads | æ³¨æ„åŠ›å¤´æ•° | [å¾…å¡«å†™] |
| n_kv_heads | KV å¤´æ•° (GQA) | [å¾…å¡«å†™] |
| head_dim | æ¯ä¸ªå¤´çš„ç»´åº¦ | [å¾…å¡«å†™] |
| n_ctx | æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ | [å¾…å¡«å†™] |

---

## ä¸‰ã€KV Cache å¤§å°è®¡ç®—

### 3.1 å…¬å¼

```
å•å±‚ KV Cache = 2 Ã— seq_len Ã— n_kv_heads Ã— head_dim Ã— dtype_size
å…¨æ¨¡å‹ KV Cache = n_layers Ã— å•å±‚ KV Cache
```

å…¶ä¸­ï¼š
- `2` = K å’Œ V ä¸¤éƒ¨åˆ†
- `dtype_size` = æ•°æ®ç±»å‹å¤§å°ï¼ˆFP16 = 2 bytesï¼‰

### 3.2 Qwen2.5-0.5B çš„ KV Cache è®¡ç®—

è¯·å®Œæˆè®¡ç®—ï¼š

```
n_layers = [å¾…å¡«å†™]
n_kv_heads = [å¾…å¡«å†™]
head_dim = [å¾…å¡«å†™]
seq_len = 2048 (å‡è®¾)
dtype_size = 2 (FP16)

å•å±‚ = 2 Ã— 2048 Ã— [n_kv_heads] Ã— [head_dim] Ã— 2 = [?] bytes
å…¨æ¨¡å‹ = [n_layers] Ã— å•å±‚ = [?] bytes = [?] MB
```

---

## å››ã€llama.cpp ä¸­çš„ KV Cache

### 4.1 ç›¸å…³ç»“æ„ä½“

è¯·åœ¨æºç ä¸­æ‰¾åˆ°å¹¶è®°å½•ï¼š

```cpp
// æ–‡ä»¶ï¼š[?]
// ç»“æ„ä½“åï¼š[?]
```

### 4.2 å…³é”®å‡½æ•°

| å‡½æ•° | ä½œç”¨ | æ–‡ä»¶ä½ç½® |
|------|------|----------|
| [å¾…å¡«å†™] | åˆ›å»º KV Cache | |
| [å¾…å¡«å†™] | è·å– KV Cache å¤§å° | |

---

## äº”ã€GQA (Grouped Query Attention)

### 5.1 ä¸ºä»€ä¹ˆ n_kv_heads â‰  n_headsï¼Ÿ

```
MHA (Multi-Head Attention): n_kv_heads = n_heads
GQA (Grouped Query Attention): n_kv_heads < n_heads
MQA (Multi-Query Attention): n_kv_heads = 1
```

### 5.2 GQA çš„å¥½å¤„

è¯·å¡«å†™ï¼š

```
[å¾…å¡«å†™]
```

---

## å…­ã€KV Cache ä¸æ˜¾å­˜

### 6.1 ä¸ºä»€ä¹ˆ KV Cache æ˜¯æ˜¾å­˜ç“¶é¢ˆï¼Ÿ

```
7B æ¨¡å‹ç¤ºä¾‹ï¼š
- n_layers = 32
- n_kv_heads = 8 (GQA)
- head_dim = 128
- seq_len = 4096
- dtype = FP16 (2 bytes)

KV Cache = 2 Ã— 32 Ã— 4096 Ã— 8 Ã— 128 Ã— 2 = [?] bytes = [?] GB
```

### 6.2 PagedAttention çš„ä½œç”¨

vLLM çš„ PagedAttention å°±æ˜¯ä¸ºäº†è§£å†³ KV Cache æ˜¾å­˜ç¢ç‰‡é—®é¢˜ï¼š

```
[å¾…å¡«å†™ï¼šç®€è¿° PagedAttention åŸç†]
```

---

## ä¸ƒã€å®è·µï¼šæ‰“å° KV Cache å¤§å°

### 7.1 ä½¿ç”¨ llama-cli æŸ¥çœ‹

```bash
# å‘½ä»¤ï¼š
[å¾…å¡«å†™]
```

### 7.2 æºç ä¸­è·å–

```cpp
// ä»£ç ç‰‡æ®µï¼š
[å¾…å¡«å†™]
```

---

## å…«ã€æ€è€ƒé—®é¢˜

1. **ä¸ºä»€ä¹ˆ seq_len å¢åŠ ä¼šæ˜¾è‘—å¢åŠ æ˜¾å­˜ä½¿ç”¨ï¼Ÿ**

   [å¾…å¡«å†™]

2. **å¦‚ä½•åœ¨æœ‰é™æ˜¾å­˜ä¸‹æ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼Ÿ**

   [å¾…å¡«å†™]

3. **KV Cache é‡åŒ– (KV Cache Quantization) æ˜¯ä»€ä¹ˆï¼Ÿ**

   [å¾…å¡«å†™]

---

## ä¹ã€å…³é”®æ•°å€¼æ€»ç»“

| æŒ‡æ ‡ | Qwen2.5-0.5B | 7B æ¨¡å‹å‚è€ƒ |
|------|--------------|-------------|
| KV Cache (1024 tokens) | [å¾…è®¡ç®—] | |
| KV Cache (4096 tokens) | [å¾…è®¡ç®—] | |
| KV Cache å æ€»æ˜¾å­˜æ¯”ä¾‹ | [å¾…è®¡ç®—] | |

---

## å‚è€ƒèµ„æ–™

- `src/llama-kv-cache.cpp` - KV Cache å®ç°
- `include/llama.h` - ç›¸å…³ API
- vLLM è®ºæ–‡: PagedAttention

---

*ğŸ“ ç¬”è®°å®Œæˆæ—¶é—´: [å¾…å¡«å†™]*

